{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f42312",
   "metadata": {},
   "source": [
    "<center><h1>DSCI-552 HOMEWORK 3</h1>\n",
    "<br>\n",
    "<font size=\"3\">Name: Vorapoom Thirapatarapong</font>\n",
    "<br>\n",
    "<font size=\"3\">USC ID: 4397330150 Github Username: bvorapoom</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6bb96f",
   "metadata": {},
   "source": [
    "# Time Series Classification Part 1: Feature Creation/Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d2c48e",
   "metadata": {},
   "source": [
    "#### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acac8898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bootstrapped.bootstrap as bs\n",
    "import bootstrapped.stats_functions as bs_stats\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330f0af",
   "metadata": {},
   "source": [
    "#### get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f33965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.DataFrame(columns = ['activities', 'dataset_number', 'df'], dtype = object)\n",
    "csv_index = {\n",
    "    'bending1': np.arange(1, 8, 1),\n",
    "    'bending2': np.arange(1, 7, 1),\n",
    "    'cycling': np.arange(1, 16, 1),\n",
    "    'lying': np.arange(1, 16, 1),\n",
    "    'sitting': np.arange(1, 16, 1),\n",
    "    'standing': np.arange(1, 16, 1),\n",
    "    'walking': np.arange(1, 16, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618af699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py:993: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.array(value)\n"
     ]
    }
   ],
   "source": [
    "for acti, ds_list in csv_index.items():\n",
    "    for ds_no in ds_list:\n",
    "        csv_path = '../data/ARem/' + acti + '/dataset' + str(ds_no) + '.csv'\n",
    "        temp_df = pd.read_csv(csv_path, skiprows = 4)\n",
    "        temp_df.columns = ['time' if col == '# Columns: time' else col for col in temp_df.columns]\n",
    "        df_main.loc[len(df_main), :] = [acti, ds_no, temp_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2c788e",
   "metadata": {},
   "source": [
    "#### the 2 sections below are to read the data into train and test set - but they are not used in these homework as the main task is to create time-domain features for all data (it will be used in HW4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4966498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_test = pd.DataFrame(columns = ['activities', 'dataset_number', 'df'], dtype = object)\n",
    "df_main_train = pd.DataFrame(columns = ['activities', 'dataset_number', 'df'], dtype = object)\n",
    "\n",
    "csv_index_test = {\n",
    "    'bending1': np.arange(1, 3, 1),\n",
    "    'bending2': np.arange(1, 3, 1),\n",
    "    'cycling': np.arange(1, 4, 1),\n",
    "    'lying': np.arange(1, 4, 1),\n",
    "    'sitting': np.arange(1, 4, 1),\n",
    "    'standing': np.arange(1, 4, 1),\n",
    "    'walking': np.arange(1, 4, 1)\n",
    "}\n",
    "\n",
    "csv_index_train = {\n",
    "    'bending1': np.arange(3, 8, 1),\n",
    "    'bending2': np.arange(3, 7, 1),\n",
    "    'cycling': np.arange(4, 16, 1),\n",
    "    'lying': np.arange(4, 16, 1),\n",
    "    'sitting': np.arange(4, 16, 1),\n",
    "    'standing': np.arange(4, 16, 1),\n",
    "    'walking': np.arange(4, 16, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb4be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading test dataset\n",
    "for acti, ds_list in csv_index_test.items():\n",
    "    for ds_no in ds_list:\n",
    "        csv_path = '../data/ARem/' + acti + '/dataset' + str(ds_no) + '.csv'\n",
    "        temp_df = pd.read_csv(csv_path, skiprows = 4)\n",
    "        temp_df.columns = ['time' if col == '# Columns: time' else col for col in temp_df.columns]\n",
    "        temp_df.set_index('time', inplace = True)\n",
    "        df_main_test.loc[len(df_main_test), :] = [acti, ds_no, temp_df]\n",
    "        \n",
    "# loading train dataset\n",
    "for acti, ds_list in csv_index_train.items():\n",
    "    for ds_no in ds_list:\n",
    "        csv_path = '../data/ARem/' + acti + '/dataset' + str(ds_no) + '.csv'\n",
    "        temp_df = pd.read_csv(csv_path, skiprows = 4)\n",
    "        temp_df.columns = ['time' if col == '# Columns: time' else col for col in temp_df.columns]\n",
    "        temp_df.set_index('time', inplace = True)\n",
    "        df_main_train.loc[len(df_main_train), :] = [acti, ds_no, temp_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba8c91",
   "metadata": {},
   "source": [
    "## (c) Feature Extraction\n",
    "## Classification of time series usually needs extracting features from them. In this problem, we focus on time-domain features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5789e882",
   "metadata": {},
   "source": [
    "### (c)-i. Research what types of time-domain features are usually used in time series classification and list them (examples are minimum, maximum, mean, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516c362",
   "metadata": {},
   "source": [
    "> - Central Tendency features: Mean, Median, Mode\n",
    "> - Minimum, Maximum, Percentile, Decile, Quartile\n",
    "> - Variability features: Standard Deviation, Variance, IQR, Range\n",
    "> - Skewness and Kurtosis\n",
    "> - Cross correlation between each dimension\n",
    "> - parameters of ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efced327",
   "metadata": {},
   "source": [
    "### (c)-ii. Extract the time-domain features minimum, maximum, mean, median, standard deviation, first quartile, and third quartile for all of the 6 time series in each instance. You are free to normalize/standardize features or use them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc009e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_time_domain_features(df):\n",
    "    target_cols = ['avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "    assert len(target_cols) == 6, 'number of target columns to extract features is not 6 !!'\n",
    "    \n",
    "    temp_list = []\n",
    "    for col in target_cols:\n",
    "        target_df = df.loc[:, col]\n",
    "        temp_min = target_df.min()\n",
    "        temp_max = target_df.max()\n",
    "        temp_mean = target_df.mean()\n",
    "        temp_median = target_df.quantile([0.5]).values[0]\n",
    "        temp_sd =  target_df.std()\n",
    "        temp_1stquart = target_df.quantile([0.25]).values[0]\n",
    "        temp_3rdquart = target_df.quantile([0.75]).values[0]\n",
    "        temp_list.extend([temp_min, temp_max, temp_mean, temp_median, temp_sd, temp_1stquart, temp_3rdquart])\n",
    "        \n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec5e6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_cols = ['instance', 'min1', 'max1', 'mean1', 'median1', 'sd1', '1st_quart1', '3rd_quart1',\n",
    "          'min2', 'max2', 'mean2', 'median2', 'sd2', '1st_quart2', '3rd_quart2',\n",
    "          'min3', 'max3', 'mean3', 'median3', 'sd3', '1st_quart3', '3rd_quart3',\n",
    "          'min4', 'max4', 'mean4', 'median4', 'sd4', '1st_quart4', '3rd_quart4',\n",
    "          'min5', 'max5', 'mean5', 'median5', 'sd5', '1st_quart5', '3rd_quart5',\n",
    "          'min6', 'max6', 'mean6', 'median6', 'sd6', '1st_quart6', '3rd_quart6',]\n",
    "df_timedomain_features = pd.DataFrame(columns = td_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4503aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in range(len(df_main)):\n",
    "    temp_df = df_main.loc[obs, 'df']\n",
    "    temp_time_domain_features = gen_time_domain_features(temp_df)\n",
    "    df_timedomain_features.loc[len(df_timedomain_features), :] = [obs + 1, *temp_time_domain_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47dd966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           min1   max1      mean1 median1       sd1 1st_quart1 3rd_quart1  \\\n",
      "instance                                                                    \n",
      "1         37.25   45.0  40.624792    40.5  1.476967      39.25       42.0   \n",
      "2          38.0  45.67  42.812812    42.5   1.43555       42.0      43.67   \n",
      "3          35.0   47.4    43.9545   44.33  1.558835       43.0       45.0   \n",
      "4          33.0  47.75  42.179813    43.5  3.670666      39.15       45.0   \n",
      "5          33.0  45.75  41.678063   41.75   2.24349      41.33      42.75   \n",
      "\n",
      "         min2  max2     mean2  ...       sd5 1st_quart5 3rd_quart5 min6  max6  \\\n",
      "instance                       ...                                              \n",
      "1         0.0   1.3  0.358604  ...  2.188449       33.0       36.0  0.0  1.92   \n",
      "2         0.0  1.22  0.372438  ...  1.995255       32.0       34.5  0.0  3.11   \n",
      "3         0.0   1.7   0.42625  ...  1.999604    35.3625       36.5  0.0  1.79   \n",
      "4         0.0   3.0  0.696042  ...  3.849448    30.4575      36.33  0.0  2.18   \n",
      "5         0.0  2.83  0.535979  ...  2.411026    28.4575      31.25  0.0  1.79   \n",
      "\n",
      "             mean6 median6       sd6 1st_quart6 3rd_quart6  \n",
      "instance                                                    \n",
      "1         0.570583    0.43  0.582915        0.0        1.3  \n",
      "2         0.571083    0.43   0.60101        0.0        1.3  \n",
      "3         0.493292    0.43  0.513506        0.0       0.94  \n",
      "4         0.613521     0.5  0.524317        0.0        1.0  \n",
      "5         0.383292    0.43  0.389164        0.0        0.5  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "df_timedomain_features.set_index('instance', inplace = True)\n",
    "print(df_timedomain_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e56fa259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              min1      max1     mean1   median1       sd1  1st_quart1  \\\n",
      "instance                                                                 \n",
      "1         0.776042  0.571429  0.685196  0.684211  0.188828    0.647355   \n",
      "2         0.791667  0.596952  0.778537  0.768421  0.183415    0.758186   \n",
      "3         0.729167  0.662857  0.827242  0.845474  0.199526    0.798489   \n",
      "4         0.687500  0.676190  0.751534  0.810526  0.475507    0.643325   \n",
      "5         0.687500  0.600000  0.730129  0.736842  0.288999    0.731184   \n",
      "\n",
      "          3rd_quart1  min2      max2     mean2  ...       sd5  1st_quart5  \\\n",
      "instance                                        ...                         \n",
      "1           0.563636   0.0  0.051755  0.076911  ...  0.072449    0.931248   \n",
      "2           0.624364   0.0  0.046996  0.079939  ...  0.040816    0.902146   \n",
      "3           0.672727   0.0  0.075550  0.091715  ...  0.041529    1.000000   \n",
      "4           0.672727   0.0  0.152885  0.150758  ...  0.344411    0.857257   \n",
      "5           0.590909   0.0  0.142772  0.115729  ...  0.108892    0.799054   \n",
      "\n",
      "          3rd_quart5  min6      max6     mean6   median6       sd6  \\\n",
      "instance                                                             \n",
      "1           0.983849   0.0  0.010998  0.059510  0.000000  0.123495   \n",
      "2           0.935395   0.0  0.111675  0.059669  0.000000  0.134981   \n",
      "3           1.000000   0.0  0.000000  0.034952  0.000000  0.079434   \n",
      "4           0.994509   0.0  0.032995  0.073153  0.024055  0.086297   \n",
      "5           0.830413   0.0  0.000000  0.000000  0.000000  0.000503   \n",
      "\n",
      "          1st_quart6  3rd_quart6  \n",
      "instance                          \n",
      "1                0.0    0.194293  \n",
      "2                0.0    0.194293  \n",
      "3                0.0    0.106861  \n",
      "4                0.0    0.121433  \n",
      "5                0.0    0.000000  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# normalize the time domain features using MinMaxScaler in order to interpret SD between each feature easier\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_timedomain_features_norm = pd.DataFrame(scaler.fit_transform(df_timedomain_features), \n",
    "                                           columns = df_timedomain_features.columns, index = df_timedomain_features.index)\n",
    "                                           \n",
    "print(df_timedomain_features_norm.head())\n",
    "                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733783a7",
   "metadata": {},
   "source": [
    "### (c)-iii. Estimate the standard deviation of each of the time-domain features you extracted from the data. Then, use Python’s bootstrapped or any other method to build a 90% bootsrap confidence interval for the standard deviation of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f21a2c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min1          0.199374\n",
       "max1          0.167404\n",
       "mean1         0.227621\n",
       "median1       0.229055\n",
       "sd1           0.231590\n",
       "1st_quart1    0.248004\n",
       "3rd_quart1    0.186870\n",
       "min2          0.000000\n",
       "max2          0.301174\n",
       "mean2         0.344502\n",
       "median2       0.331513\n",
       "sd2           0.343955\n",
       "1st_quart2    0.329751\n",
       "3rd_quart2    0.348119\n",
       "min3          0.347819\n",
       "max3          0.205963\n",
       "mean3         0.225172\n",
       "median3       0.218184\n",
       "sd3           0.160646\n",
       "1st_quart3    0.234481\n",
       "3rd_quart3    0.227058\n",
       "min4          0.000000\n",
       "max4          0.244527\n",
       "mean4         0.384436\n",
       "median4       0.357996\n",
       "sd4           0.315466\n",
       "1st_quart4    0.388765\n",
       "3rd_quart4    0.391059\n",
       "min5          0.211172\n",
       "max5          0.202656\n",
       "mean5         0.181419\n",
       "median5       0.172260\n",
       "sd5           0.167811\n",
       "1st_quart5    0.177416\n",
       "3rd_quart5    0.178688\n",
       "min6          0.106600\n",
       "max6          0.213107\n",
       "mean6         0.366932\n",
       "median6       0.373359\n",
       "sd6           0.328581\n",
       "1st_quart6    0.338653\n",
       "3rd_quart6    0.370030\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard deviation of each time-domain features\n",
    "df_timedomain_features_norm.std(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ff184a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% bootstrap confidence interval for each of the time-domain features\n",
    "\n",
    "# changing dtype of the df to float in order to pass to bootstrapped\n",
    "df_timedomain_features_norm = df_timedomain_features_norm.astype(float)\n",
    "\n",
    "df_bootstrap_std = pd.DataFrame(columns = ['time_domain_features', 'lower_bound_bs_std', 'upper_bound_bs_std'])\n",
    "\n",
    "# loop through each df columns and get bootstrap STD at 90% ci\n",
    "for col in df_timedomain_features_norm.columns:\n",
    "    bs_result = bs.bootstrap(np.array(df_timedomain_features_norm.loc[:, col]), stat_func = bs_stats.std, alpha = 0.1)\n",
    "    df_bootstrap_std.loc[len(df_bootstrap_std), :] = [col, bs_result.lower_bound, bs_result.upper_bound]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3e2a45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   time_domain_features lower_bound_bs_std upper_bound_bs_std\n",
      "0                  min1            0.17319           0.225515\n",
      "1                  max1           0.132749           0.206831\n",
      "2                 mean1           0.202716           0.253303\n",
      "3               median1           0.204308           0.255122\n",
      "4                   sd1           0.207788            0.25661\n",
      "5            1st_quart1           0.227062           0.269806\n",
      "6            3rd_quart1           0.160146            0.21528\n",
      "7                  min2                0.0                0.0\n",
      "8                  max2           0.278728           0.325044\n",
      "9                 mean2           0.314252           0.381121\n",
      "10              median2           0.298733            0.37065\n",
      "11                  sd2           0.319988           0.373711\n",
      "12           1st_quart2           0.297304           0.367699\n",
      "13           3rd_quart2           0.318869           0.384474\n",
      "14                 min3           0.328473           0.369152\n",
      "15                 max3           0.179684           0.234306\n",
      "16                mean3           0.195957           0.257084\n",
      "17              median3             0.1895           0.249227\n",
      "18                  sd3           0.129982           0.190876\n",
      "19           1st_quart3           0.206439           0.265867\n",
      "20           3rd_quart3           0.198257           0.259994\n",
      "21                 min4                0.0                0.0\n",
      "22                 max4           0.223303           0.266372\n",
      "23                mean4           0.363717           0.411286\n",
      "24              median4           0.338512           0.384329\n",
      "25                  sd4            0.29488           0.339423\n",
      "26           1st_quart4           0.364858           0.417294\n",
      "27           3rd_quart4           0.369972           0.418155\n",
      "28                 min5           0.161934           0.269265\n",
      "29                 max5           0.172365           0.236246\n",
      "30                mean5           0.146442           0.219483\n",
      "31              median5            0.13844           0.209281\n",
      "32                  sd5           0.135575           0.200999\n",
      "33           1st_quart5           0.144075           0.214508\n",
      "34           3rd_quart5           0.145245           0.215359\n",
      "35                 min6           0.030523           0.211986\n",
      "36                 max6           0.191079           0.234535\n",
      "37                mean6           0.345612           0.394535\n",
      "38              median6           0.350162           0.403133\n",
      "39                  sd6            0.30937           0.351485\n",
      "40           1st_quart6           0.314761           0.367941\n",
      "41           3rd_quart6           0.349094           0.396632\n"
     ]
    }
   ],
   "source": [
    "print(df_bootstrap_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e82c86",
   "metadata": {},
   "source": [
    "### (c)-iv. Use your judgement to select the three most important time-domain features (one option may be min, mean, and max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c9b0d",
   "metadata": {},
   "source": [
    "> Three most important time-domain features I chose are <b>Mean, Median, and Standard Deviation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f748f59",
   "metadata": {},
   "source": [
    "> Few reasons to support the selection\n",
    "> - All three features show high SD and bootstrap SD after normalization, which mean there could be more information we can use in order for the classification problem.\n",
    "> - Mean and Median are both chosen as they're showing the central tendency of the data. Moreover, both of which could give more information on the skewness of the data\n",
    "> - SD is chosen as it gives the variability of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745050be",
   "metadata": {},
   "source": [
    "> Few reasons to not select other features\n",
    "> - Min and Max are not chosen because they are more likely to be affected by outliers, especially in this case where we did not detect and handle outliers\n",
    "> - For feature 2, 4, and 6, the SD and bootstrap SD of Min feature show that there is low variation in the Min values so we're not expected to draw much information from this feature\n",
    "> - 1st and 3rd quartiles are as good as SD in my opinion as their SD and bootstrap SD are high, meaning there could be information we could get in order to classify the activities. Moreover, they both represent the variability of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffb3cd",
   "metadata": {},
   "source": [
    "# ISLR 3.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50e537",
   "metadata": {},
   "source": [
    "I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. Y = β0 +β1X +β2X2 +β3X3 +ε."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40244fef",
   "metadata": {},
   "source": [
    "(a) Suppose that the true relationship between X and Y is linear, i.e. Y = β0 + β1X + ε. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad0831",
   "metadata": {},
   "source": [
    "> We'd expect the training RSS for cubic regression to be lower. Even though the cubic regression might cause overfitting, the higher flexibility of the cubic model should be able to capture noises in the training dataset better and therefore result in lower training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c3803",
   "metadata": {},
   "source": [
    "(b) Answer (a) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b05166",
   "metadata": {},
   "source": [
    "> We'd expect the testing RSS for linear RSS to be lower. Although the cubic regression might perform better on the training set because of its higher flexibility, that is more prone to overfitting and therefore fail to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a0f2e1",
   "metadata": {},
   "source": [
    "(c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850923a",
   "metadata": {},
   "source": [
    "> We'd expect the traning RSS for cubic regression to be lower. Even though we don't konw how far the actual data is from linear, the higher flexibility of the cubic model would be able to fit itself to the training data better than the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580bb48",
   "metadata": {},
   "source": [
    "(d) Answer (c) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa04162",
   "metadata": {},
   "source": [
    "> Since it is unclear how far the actual data is from linear, we cannot tell which model will perform better. In case it is closer to linear than cubic, the linear model should give lower testing RSS and vice versa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
