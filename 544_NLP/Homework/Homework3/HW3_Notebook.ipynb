{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e587efcd",
   "metadata": {},
   "source": [
    "<center><h1>CSCI-544 HOMEWORK 3</h1>\n",
    "<br>\n",
    "<center><font size=\"3\">Name: Vorapoom Thirapatarapong</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b92d4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "[nltk_data] Downloading package wordnet to /Users/boom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/boom/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/boom/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "import gensim.downloader as api\n",
    "import gensim.models\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b7aeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f820f478cf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters\n",
    "r_state = 555\n",
    "sample_size = 20000\n",
    "test_ratio = 0.2\n",
    "torch.manual_seed(r_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae1da0",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647fabf",
   "metadata": {},
   "source": [
    "### Load data + Initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b58b03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-f74a6e2e7113>:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data.tsv', sep='\\t', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "# read dataset\n",
    "df = pd.read_csv('data.tsv', sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a728295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean invalid rows & format data types\n",
    "df = df.loc[:, ['review_body', 'star_rating']]\n",
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "df = df[~df['star_rating'].isna()]\n",
    "df = df[~df['review_body'].isna()]\n",
    "df['star_rating'] = df['star_rating'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d330ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns selection\n",
    "df = df.loc[:, ['review_body', 'star_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b29128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e592d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group ratings to 3 classes\n",
    "mapping = {1: 0, 2: 0, 3: 1, 4: 2, 5: 2}\n",
    "df = df.replace({'star_rating': mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148bb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of the ,000 sample reviews for training custom Word2Vec\n",
    "list_w2v = [df[df.star_rating == 0].sample(n=20000, random_state=r_state),\n",
    "            df[df.star_rating == 1].sample(n=20000, random_state=r_state),\n",
    "            df[df.star_rating == 2].sample(n=20000, random_state=r_state)]\n",
    "df_w2v = pd.concat(list_w2v)['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e463d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a balanced dataset of 60k reviews\n",
    "list_sample = [df[df.star_rating == 0].sample(n=sample_size, random_state=r_state),\n",
    "            df[df.star_rating == 1].sample(n=sample_size, random_state=r_state),\n",
    "            df[df.star_rating == 2].sample(n=sample_size, random_state=r_state)]\n",
    "df_sample = pd.concat(list_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "559801d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns renaming\n",
    "df_sample.columns = ['review', 'stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e677eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       review\n",
      "stars        \n",
      "0       20000\n",
      "1       20000\n",
      "2       20000\n"
     ]
    }
   ],
   "source": [
    "# verify number of samples\n",
    "print(df_sample.groupby('stars').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b5304",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23659bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text)\n",
    "\n",
    "def perform_contractions(text):\n",
    "    return ' '.join([contractions.fix(word) for word in text.split()])\n",
    "\n",
    "def remove_non_alpha_chars(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d729ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_sample.copy()\n",
    "\n",
    "# lowercase\n",
    "df_clean['review'] = df_clean['review'].apply(str.lower)\n",
    "\n",
    "# remove HTML\n",
    "df_clean['review'] = df_clean['review'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "\n",
    "# remove URLs\n",
    "df_clean['review'] = df_clean['review'].apply(remove_urls)\n",
    "\n",
    "# contractions\n",
    "df_clean['review'] = df_clean['review'].apply(perform_contractions)\n",
    "\n",
    "# non-alphabet chars\n",
    "df_clean['review'] = df_clean['review'].apply(remove_non_alpha_chars)\n",
    "\n",
    "# extra spaces\n",
    "df_clean['review'] = df_clean['review'].apply(remove_extra_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809bc74",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e9ea444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def perform_lemmatization(lemmatizer, text):\n",
    "    lemmatized_list = []\n",
    "    for word, pos_tag in nltk.pos_tag(text.split()):\n",
    "        if pos_tag.startswith('V'):\n",
    "            lemmatized_list.append(lemmatizer.lemmatize(word, 'v'))\n",
    "        elif pos_tag.startswith('J'):\n",
    "            lemmatized_list.append(lemmatizer.lemmatize(word, 'a'))\n",
    "        else:\n",
    "            lemmatized_list.append(lemmatizer.lemmatize(word))\n",
    "    return ' '.join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d15972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df_preproc = df_clean.copy()\n",
    "df_preproc['review'] = df_preproc['review'].apply(remove_stop_words)\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_lemma = df_preproc.copy()\n",
    "df_lemma['review'] = df_lemma['review'].apply(lambda x: perform_lemmatization(lemmatizer, x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42207f6b",
   "metadata": {},
   "source": [
    "### Split Training / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d451ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "df_X = df_lemma['review'].reset_index(drop=True)\n",
    "df_y = df_lemma['stars'].reset_index(drop=True)\n",
    "\n",
    "# split training/testing = 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=test_ratio, random_state=r_state, stratify=df_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f55279",
   "metadata": {},
   "source": [
    "### TF-IDF features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b725b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "df_X_tfidf_pre = pd.concat([X_train, X_test])\n",
    "tfidf = vectorizer.fit_transform(df_X_tfidf_pre)\n",
    "df_X_tfidf = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "X_train_tfidf = df_X_tfidf[:int(3*sample_size*(1-test_ratio))]\n",
    "X_test_tfidf = df_X_tfidf[int(3*sample_size*(1-test_ratio)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6f779",
   "metadata": {},
   "source": [
    "# 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90303064",
   "metadata": {},
   "source": [
    "### 2(a). Pre-trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ccc12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30fd5552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5111032\n"
     ]
    }
   ],
   "source": [
    "# Semantics similarity example #1\n",
    "\n",
    "a = wv['boat'] - wv['water'] + wv['air']\n",
    "b = wv['plane']\n",
    "\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "434ede71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76435417\n"
     ]
    }
   ],
   "source": [
    "# Semantics similarity example #2\n",
    "\n",
    "a = wv['sea']\n",
    "b = wv['ocean']\n",
    "\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e6ecd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023547666\n"
     ]
    }
   ],
   "source": [
    "# Semantics similarity example #3\n",
    "\n",
    "a = wv['vegetable']\n",
    "b = wv['star']\n",
    "\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d899c22",
   "metadata": {},
   "source": [
    "### 2(b). Custom Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80c754f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus using 500,000 reviews\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        for index, row in df_w2v.iteritems():\n",
    "#             print(row)\n",
    "            yield utils.simple_preprocess(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9534b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-2c0d12710906>:6: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for index, row in df_w2v.iteritems():\n"
     ]
    }
   ],
   "source": [
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences, min_count=9, vector_size=300, window=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb6b2895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.046374116\n"
     ]
    }
   ],
   "source": [
    "# Semantics similarity example #1\n",
    "\n",
    "a = model.wv['boat'] - model.wv['water'] + model.wv['air']\n",
    "b = model.wv['plane']\n",
    "\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d70c1fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59367234\n"
     ]
    }
   ],
   "source": [
    "# Semantics similarity example #2\n",
    "\n",
    "a = model.wv['sea']\n",
    "b = model.wv['ocean']\n",
    "\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0db3967e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09536163\n"
     ]
    }
   ],
   "source": [
    "# Semantics similarity example #3\n",
    "\n",
    "a = model.wv['vegetable']\n",
    "b = model.wv['star']\n",
    "\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8aace7",
   "metadata": {},
   "source": [
    "> What do you conclude from comparing vectors generated by yourself and the pretrained model? <br>\n",
    "The dimensions of the vectors from both pretrained(word2vec-google-news-300) and custom Word2Vec models are the same which is 300. However, the pretrained model is able to encode the semantics similarity much better - when looking at two similar words, the pretrained model is able to give higher similarity score & when looking at 2 words with no relationship at all, the pretrained model is able to give a slightly lower similarity score. This might be because that the traning samples of the pretrained model is much larger which makes it generalizes better.\n",
    "\n",
    "> Which of the Word2Vec models seems to encode semantic similarities between words better? <br>\n",
    "From the 3 examples chosen, it seems like the pretrained Word2Vec model is able to encode the semantic similarities better. For the first 2 examples, the two objects are highly related which should give higher similarity value, and the pretrained model is giving higher similarities. For the third example which is the two objects that are not related, both models seem to satisfy the condition where they give relatively low similarity scores between the two objects. The reason for better semantic similarity encoding for the pretrained model might be a much larger size of training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a864ca",
   "metadata": {},
   "source": [
    "# 3. Simple Models: Single perceptron & SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ccfa13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for W2V Feature extraction\n",
    "def extract_features_w2v(data, max_vec_concat, max_vec_rnn):\n",
    "    data = list(data)\n",
    "    list_output_avg = list()\n",
    "    list_output_concat = list()\n",
    "    df_output_concat = pd.DataFrame(columns=['c'+str(i) for i in range(1, 300*max_vec_concat+1)])\n",
    "    np_output_3d = np.zeros((len(data), max_vec_rnn, 300), dtype=np.float32)\n",
    "    x = 0\n",
    "    time_bf, time_start = time.time(), time.time()\n",
    "    \n",
    "    for i, instance in enumerate(data):\n",
    "        \n",
    "        cnt = 0\n",
    "        list_temp = []\n",
    "        list_temp2 = []\n",
    "        for word in instance.split():\n",
    "            try:\n",
    "                vec = wv[word]\n",
    "                list_temp2.append(list(vec))\n",
    "                list_temp = list_temp + list(vec)\n",
    "                if cnt < 20:\n",
    "                    np_output_3d[i, cnt, :] = vec\n",
    "                    cnt += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        temp_avg = np.mean(list_temp2, axis=0)\n",
    "        list_output_avg.append(temp_avg)\n",
    "        list_temp = (list_temp + max_vec_concat*300*[0.0])[:max_vec_concat*300]\n",
    "        list_output_concat.append(list_temp)\n",
    "        \n",
    "        x += 1\n",
    "        if x % 2000 == 0: \n",
    "            print(x, time.time() - time_bf)\n",
    "            time_bf = time.time()\n",
    "            \n",
    "    list_output_avg = [x if isinstance(x, np.ndarray) else np.zeros(300) for x in list_output_avg]\n",
    "    list_output_concat = [x if len(x) > 0 else [0.0]*3000 for x in list_output_concat ]\n",
    "            \n",
    "    df_output_avg = pd.DataFrame(list_output_avg, columns=['c'+str(i) for i in range(1, 301)])\n",
    "    df_output_concat = pd.DataFrame(list_output_concat, columns=['c'+str(i) for i in range(1, 300*max_vec_concat+1)])\n",
    "        \n",
    "    print(time.time() - time_start)\n",
    "        \n",
    "    return df_output_avg, df_output_concat, np_output_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290e295",
   "metadata": {},
   "source": [
    "### 3(a). Single perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbec1f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 5.053712844848633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 5.233086109161377\n",
      "6000 5.562352895736694\n",
      "8000 6.524694919586182\n",
      "10000 6.205254793167114\n",
      "12000 5.549623250961304\n",
      "14000 6.020545959472656\n",
      "16000 5.951589822769165\n",
      "18000 4.73419976234436\n",
      "20000 5.9541168212890625\n",
      "22000 4.642481088638306\n",
      "24000 5.937429666519165\n",
      "26000 5.508419036865234\n",
      "28000 6.115911245346069\n",
      "30000 5.167878150939941\n",
      "32000 5.232169151306152\n",
      "34000 5.269054889678955\n",
      "36000 5.526584148406982\n",
      "38000 6.3384950160980225\n",
      "40000 5.78859806060791\n",
      "42000 6.483400821685791\n",
      "44000 4.395051002502441\n",
      "46000 5.530379056930542\n",
      "48000 5.462535858154297\n",
      "192.25431990623474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 5.040863037109375\n",
      "4000 4.650351285934448\n",
      "6000 6.271064043045044\n",
      "8000 6.033645153045654\n",
      "10000 5.058319807052612\n",
      "12000 4.746193885803223\n",
      "43.062626123428345\n"
     ]
    }
   ],
   "source": [
    "# extract Word2Vec features\n",
    "X_train_w2v, X_train_w2v_concat, X_train_3d = extract_features_w2v(X_train, 10, 20)\n",
    "X_test_w2v, X_test_w2v_concat, X_test_3d = extract_features_w2v(X_test, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95fa89ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v.fillna(0, inplace=True)\n",
    "X_train_w2v_concat.fillna(0, inplace=True)\n",
    "X_test_w2v.fillna(0, inplace=True)\n",
    "X_test_w2v_concat.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b508f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (TFIDF, Perceptron) = 0.6088333333333333\n"
     ]
    }
   ],
   "source": [
    "# TFIDF features\n",
    "clf_perceptron = Perceptron(random_state=r_state, penalty='elasticnet')\n",
    "clf_perceptron.fit(X_train_tfidf, y_train)\n",
    "acc_tfidf_perceptron = clf_perceptron.score(X_test_tfidf, y_test)\n",
    "print('Accuracy (TFIDF, Perceptron) =', acc_tfidf_perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cd17b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (W2V, Perceptron) = 0.5390833333333334\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec features\n",
    "clf_perceptron = Perceptron(random_state=r_state, penalty='elasticnet')\n",
    "clf_perceptron.fit(X_train_w2v, y_train)\n",
    "acc_w2v_perceptron = clf_perceptron.score(X_test_w2v, y_test)\n",
    "print('Accuracy (W2V, Perceptron) =', acc_w2v_perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d152271",
   "metadata": {},
   "source": [
    "### 3(b). SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63192d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (TFIDF, SVM) = 0.65425\n"
     ]
    }
   ],
   "source": [
    "# TFIDF features\n",
    "clf_SVC = LinearSVC(random_state=r_state, multi_class='ovr', dual=True, max_iter=50000)\n",
    "clf_SVC.fit(X_train_tfidf, y_train)\n",
    "acc_tfidf_svm = clf_SVC.score(X_test_tfidf, y_test)\n",
    "print('Accuracy (TFIDF, SVM) =', acc_tfidf_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83b752d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (W2V, SVM) = 0.62775\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec features\n",
    "clf_SVC = LinearSVC(random_state=r_state, multi_class='ovr', dual=True, max_iter=50000)\n",
    "clf_SVC.fit(X_train_w2v, y_train)\n",
    "acc_w2v_svm = clf_SVC.score(X_test_w2v, y_test)\n",
    "print('Accuracy (W2V, SVM) =', acc_w2v_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c2b32",
   "metadata": {},
   "source": [
    "> What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)? <br>\n",
    "The accuracy values from the models that use Word2Vec as input features are much lower using perceptron and slightly lower using SVM, meaning that word-level feature extraction in this case is less meaningful for model training than using a document-level features. My assumption on this behavior is that using average Word2Vec as features are not telling anything about the sentence as a whole - the effect of the keyword that might tell the sentiment got diluted from averaging all the words / it does not consider the sequence of the words or the long-term dependencies / etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b40c99",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36821201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train_w2v.to_numpy().astype(np.float32))\n",
    "X_test_tensor = torch.from_numpy(X_test_w2v.to_numpy().astype(np.float32))\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values)\n",
    "y_test_tensor = torch.tensor(y_test.values)\n",
    "\n",
    "# Passing to DataLoader\n",
    "train_tensor = data_utils.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = data_utils.DataLoader(train_tensor, batch_size=10, shuffle=True)\n",
    "\n",
    "test_tensor = data_utils.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = data_utils.DataLoader(test_tensor, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bad1f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(FNN, self).__init__()\n",
    "        # number of hidden nodes in each layer\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_features, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "754eb07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(y_pred, y_true):\n",
    "    max_scores, max_idx_class = y_pred.max(dim=1)\n",
    "    num_match = (y_true == max_idx_class).sum().item()\n",
    "    acc = num_match / y_true.size(0)\n",
    "    return num_match, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32f800c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, model, train_loader, valid_loader, optimizer, criterion, print_every=1):\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "    len_train = len(train_loader.dataset)\n",
    "    len_valid = len(valid_loader.dataset)\n",
    "    best_acc = -np.Inf\n",
    "    best_sd = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()    # *data.size(0)\n",
    "            # update training accuracy\n",
    "            num_match, _ = get_acc(output, target)\n",
    "            train_acc += num_match\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        with torch.no_grad():\n",
    "            for data, target in valid_loader:\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(data)\n",
    "                # calculate the loss\n",
    "                loss = criterion(output, target)\n",
    "                # update running validation loss \n",
    "                valid_loss += loss.item()\n",
    "                # update training accuracy\n",
    "                num_match, _ = get_acc(output, target)\n",
    "                valid_acc += num_match\n",
    "            \n",
    "        train_acc = train_acc / len_train\n",
    "        valid_acc = valid_acc / len_valid\n",
    "        \n",
    "        train_loss = train_loss / len_train * 10\n",
    "        valid_loss = valid_loss / len_valid * 10\n",
    "        \n",
    "        # save params of the best model\n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_sd = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: {} \\tTrain Loss: {:.4f}\\tValid Loss: {:.4f}\\tTrain Acc: {:.4f}\\tValid Acc: {:.4f}'.format(\n",
    "                epoch+1, train_loss, valid_loss, train_acc, valid_acc))\n",
    "            \n",
    "    return best_sd\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed9f43",
   "metadata": {},
   "source": [
    "### 4(a). Average Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4dc5ba67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize the NN\n",
    "model = FNN(300)\n",
    "print(model)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d150de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTrain Loss: 0.9719\tValid Loss: 0.8644\tTrain Acc: 0.4985\tValid Acc: 0.5916\n",
      "Epoch: 2 \tTrain Loss: 0.8584\tValid Loss: 0.8527\tTrain Acc: 0.6022\tValid Acc: 0.6048\n",
      "Epoch: 3 \tTrain Loss: 0.8387\tValid Loss: 0.8933\tTrain Acc: 0.6128\tValid Acc: 0.5831\n",
      "Epoch: 4 \tTrain Loss: 0.8272\tValid Loss: 0.8182\tTrain Acc: 0.6199\tValid Acc: 0.6283\n",
      "Epoch: 5 \tTrain Loss: 0.8194\tValid Loss: 0.8260\tTrain Acc: 0.6244\tValid Acc: 0.6246\n",
      "Epoch: 6 \tTrain Loss: 0.8133\tValid Loss: 0.8310\tTrain Acc: 0.6272\tValid Acc: 0.6168\n",
      "Epoch: 7 \tTrain Loss: 0.8065\tValid Loss: 0.8122\tTrain Acc: 0.6315\tValid Acc: 0.6279\n",
      "Epoch: 8 \tTrain Loss: 0.8002\tValid Loss: 0.8073\tTrain Acc: 0.6338\tValid Acc: 0.6306\n",
      "Epoch: 9 \tTrain Loss: 0.7949\tValid Loss: 0.8278\tTrain Acc: 0.6380\tValid Acc: 0.6217\n",
      "Epoch: 10 \tTrain Loss: 0.7901\tValid Loss: 0.8030\tTrain Acc: 0.6395\tValid Acc: 0.6325\n",
      "Epoch: 11 \tTrain Loss: 0.7849\tValid Loss: 0.8123\tTrain Acc: 0.6435\tValid Acc: 0.6250\n",
      "Epoch: 12 \tTrain Loss: 0.7806\tValid Loss: 0.8053\tTrain Acc: 0.6441\tValid Acc: 0.6327\n",
      "Epoch: 13 \tTrain Loss: 0.7759\tValid Loss: 0.8014\tTrain Acc: 0.6488\tValid Acc: 0.6347\n",
      "Epoch: 14 \tTrain Loss: 0.7711\tValid Loss: 0.8129\tTrain Acc: 0.6501\tValid Acc: 0.6293\n",
      "Epoch: 15 \tTrain Loss: 0.7659\tValid Loss: 0.8011\tTrain Acc: 0.6509\tValid Acc: 0.6406\n",
      "Epoch: 16 \tTrain Loss: 0.7606\tValid Loss: 0.8076\tTrain Acc: 0.6534\tValid Acc: 0.6359\n",
      "Epoch: 17 \tTrain Loss: 0.7572\tValid Loss: 0.8144\tTrain Acc: 0.6596\tValid Acc: 0.6327\n",
      "Epoch: 18 \tTrain Loss: 0.7530\tValid Loss: 0.8047\tTrain Acc: 0.6619\tValid Acc: 0.6387\n",
      "Epoch: 19 \tTrain Loss: 0.7479\tValid Loss: 0.8076\tTrain Acc: 0.6616\tValid Acc: 0.6357\n",
      "Epoch: 20 \tTrain Loss: 0.7450\tValid Loss: 0.8027\tTrain Acc: 0.6633\tValid Acc: 0.6388\n",
      "Epoch: 21 \tTrain Loss: 0.7381\tValid Loss: 0.8125\tTrain Acc: 0.6713\tValid Acc: 0.6273\n",
      "Epoch: 22 \tTrain Loss: 0.7350\tValid Loss: 0.8501\tTrain Acc: 0.6700\tValid Acc: 0.6061\n",
      "Epoch: 23 \tTrain Loss: 0.7309\tValid Loss: 0.8092\tTrain Acc: 0.6730\tValid Acc: 0.6324\n",
      "Epoch: 24 \tTrain Loss: 0.7250\tValid Loss: 0.8080\tTrain Acc: 0.6766\tValid Acc: 0.6336\n",
      "Epoch: 25 \tTrain Loss: 0.7213\tValid Loss: 0.8086\tTrain Acc: 0.6770\tValid Acc: 0.6361\n",
      "Epoch: 26 \tTrain Loss: 0.7146\tValid Loss: 0.8282\tTrain Acc: 0.6834\tValid Acc: 0.6270\n",
      "Epoch: 27 \tTrain Loss: 0.7116\tValid Loss: 0.8224\tTrain Acc: 0.6798\tValid Acc: 0.6286\n",
      "Epoch: 28 \tTrain Loss: 0.7079\tValid Loss: 0.8150\tTrain Acc: 0.6843\tValid Acc: 0.6324\n",
      "Epoch: 29 \tTrain Loss: 0.7034\tValid Loss: 0.8204\tTrain Acc: 0.6885\tValid Acc: 0.6360\n",
      "Epoch: 30 \tTrain Loss: 0.6973\tValid Loss: 0.9531\tTrain Acc: 0.6915\tValid Acc: 0.5795\n"
     ]
    }
   ],
   "source": [
    "best_sd = train(30, model, train_loader, test_loader, optimizer, criterion, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ccc3161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN using average Word2Vec vectors:\n",
      "Training Accuracy = 0.6687916666666667\n",
      "Testing Accuracy = 0.6405833333333333\n"
     ]
    }
   ],
   "source": [
    "model_best_fnn_avg = FNN(300)\n",
    "model_best_fnn_avg.load_state_dict(best_sd)\n",
    "\n",
    "_, train_acc = get_acc(model_best_fnn_avg(X_train_tensor), y_train_tensor)\n",
    "_, test_acc = get_acc(model_best_fnn_avg(X_test_tensor), y_test_tensor)\n",
    "\n",
    "print('FNN using average Word2Vec vectors:')\n",
    "print('Training Accuracy =', train_acc)\n",
    "print('Testing Accuracy =', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413d2a1",
   "metadata": {},
   "source": [
    "### 4(b). Concatenated Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56c1e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to tensors\n",
    "X_train_tensor_concat = torch.from_numpy(X_train_w2v_concat.to_numpy().astype(np.float32))\n",
    "X_test_tensor_concat = torch.from_numpy(X_test_w2v_concat.to_numpy().astype(np.float32))\n",
    "\n",
    "# Passing to DataLoader\n",
    "train_tensor = data_utils.TensorDataset(X_train_tensor_concat, y_train_tensor)\n",
    "train_loader = data_utils.DataLoader(train_tensor, batch_size=10, shuffle=True)\n",
    "\n",
    "test_tensor = data_utils.TensorDataset(X_test_tensor_concat, y_test_tensor)\n",
    "test_loader = data_utils.DataLoader(test_tensor, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f111e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize the NN\n",
    "model = FNN(3000)\n",
    "print(model)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f7c6d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTrain Loss: 1.0627\tValid Loss: 0.9910\tTrain Acc: 0.4349\tValid Acc: 0.4943\n",
      "Epoch: 2 \tTrain Loss: 0.9529\tValid Loss: 0.9380\tTrain Acc: 0.5219\tValid Acc: 0.5373\n",
      "Epoch: 3 \tTrain Loss: 0.9135\tValid Loss: 0.9217\tTrain Acc: 0.5595\tValid Acc: 0.5558\n",
      "Epoch: 4 \tTrain Loss: 0.8848\tValid Loss: 0.9114\tTrain Acc: 0.5817\tValid Acc: 0.5657\n",
      "Epoch: 5 \tTrain Loss: 0.8646\tValid Loss: 0.9025\tTrain Acc: 0.5958\tValid Acc: 0.5694\n",
      "Epoch: 6 \tTrain Loss: 0.8459\tValid Loss: 0.9021\tTrain Acc: 0.6082\tValid Acc: 0.5660\n",
      "Epoch: 7 \tTrain Loss: 0.8275\tValid Loss: 0.9084\tTrain Acc: 0.6183\tValid Acc: 0.5675\n",
      "Epoch: 8 \tTrain Loss: 0.8050\tValid Loss: 0.9111\tTrain Acc: 0.6318\tValid Acc: 0.5700\n",
      "Epoch: 9 \tTrain Loss: 0.7787\tValid Loss: 0.9102\tTrain Acc: 0.6486\tValid Acc: 0.5653\n",
      "Epoch: 10 \tTrain Loss: 0.7435\tValid Loss: 0.9413\tTrain Acc: 0.6705\tValid Acc: 0.5614\n",
      "Epoch: 11 \tTrain Loss: 0.7002\tValid Loss: 0.9490\tTrain Acc: 0.6959\tValid Acc: 0.5589\n",
      "Epoch: 12 \tTrain Loss: 0.6453\tValid Loss: 0.9825\tTrain Acc: 0.7289\tValid Acc: 0.5584\n",
      "Epoch: 13 \tTrain Loss: 0.5747\tValid Loss: 1.0422\tTrain Acc: 0.7694\tValid Acc: 0.5488\n",
      "Epoch: 14 \tTrain Loss: 0.4997\tValid Loss: 1.1095\tTrain Acc: 0.8083\tValid Acc: 0.5472\n",
      "Epoch: 15 \tTrain Loss: 0.4153\tValid Loss: 1.2141\tTrain Acc: 0.8465\tValid Acc: 0.5347\n",
      "Epoch: 16 \tTrain Loss: 0.3360\tValid Loss: 1.3394\tTrain Acc: 0.8821\tValid Acc: 0.5331\n",
      "Epoch: 17 \tTrain Loss: 0.2590\tValid Loss: 1.4817\tTrain Acc: 0.9157\tValid Acc: 0.5302\n",
      "Epoch: 18 \tTrain Loss: 0.1949\tValid Loss: 1.6066\tTrain Acc: 0.9412\tValid Acc: 0.5248\n",
      "Epoch: 19 \tTrain Loss: 0.1425\tValid Loss: 1.7546\tTrain Acc: 0.9603\tValid Acc: 0.5288\n",
      "Epoch: 20 \tTrain Loss: 0.1011\tValid Loss: 1.8605\tTrain Acc: 0.9734\tValid Acc: 0.5292\n"
     ]
    }
   ],
   "source": [
    "best_sd = train(20, model, train_loader, test_loader, optimizer, criterion, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3485d018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN using concat Word2Vec vectors:\n",
      "Training Accuracy = 0.664125\n",
      "Testing Accuracy = 0.57\n"
     ]
    }
   ],
   "source": [
    "model_best_fnn_concat = FNN(3000)\n",
    "model_best_fnn_concat.load_state_dict(best_sd)\n",
    "\n",
    "_, train_acc = get_acc(model_best_fnn_concat(X_train_tensor_concat), y_train_tensor)\n",
    "_, test_acc = get_acc(model_best_fnn_concat(X_test_tensor_concat), y_test_tensor)\n",
    "\n",
    "print('FNN using concat Word2Vec vectors:')\n",
    "print('Training Accuracy =', train_acc)\n",
    "print('Testing Accuracy =', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f5f89",
   "metadata": {},
   "source": [
    "> What do you conclude by comparing accuracy values you obtain with those obtained in the “Simple Models” section?\n",
    "> - Considering only models using Word2Vec vectors as input features, FNN (with average W2V) gives higher accuracy which is as expected since the model is more complex than using just a single perceptron.\n",
    "> - When considering different types of input W2V vectors for FNN, model using average W2V is better than using concatenated W2V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5a400",
   "metadata": {},
   "source": [
    "# 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f37748",
   "metadata": {},
   "source": [
    "### 5(a). Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8314edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to tensors\n",
    "X_train_tensor_3d = torch.from_numpy(X_train_3d)\n",
    "X_test_tensor_3d = torch.from_numpy(X_test_3d)\n",
    "\n",
    "# Passing to DataLoader\n",
    "train_tensor = data_utils.TensorDataset(X_train_tensor_3d, y_train_tensor)\n",
    "train_loader = data_utils.DataLoader(train_tensor, batch_size=10, shuffle=False)\n",
    "\n",
    "test_tensor = data_utils.TensorDataset(X_test_tensor_3d, y_test_tensor)\n",
    "test_loader = data_utils.DataLoader(test_tensor, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8016b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_RNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_layers):\n",
    "        super(model_RNN, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size=300, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(hidden[-1])\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc40c7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_RNN(\n",
      "  (rnn): RNN(300, 20, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize the NN\n",
    "model = model_RNN(20, 2)\n",
    "print(model)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9c7258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTrain Loss: 1.1003\tValid Loss: 1.0990\tTrain Acc: 0.3327\tValid Acc: 0.3372\n",
      "Epoch: 2 \tTrain Loss: 1.0980\tValid Loss: 1.0975\tTrain Acc: 0.3436\tValid Acc: 0.3417\n",
      "Epoch: 3 \tTrain Loss: 1.0948\tValid Loss: 1.0812\tTrain Acc: 0.3575\tValid Acc: 0.4044\n",
      "Epoch: 4 \tTrain Loss: 1.0049\tValid Loss: 0.9439\tTrain Acc: 0.4827\tValid Acc: 0.5282\n",
      "Epoch: 5 \tTrain Loss: 0.9472\tValid Loss: 0.9196\tTrain Acc: 0.5256\tValid Acc: 0.5492\n",
      "Epoch: 6 \tTrain Loss: 0.9339\tValid Loss: 0.9110\tTrain Acc: 0.5381\tValid Acc: 0.5558\n",
      "Epoch: 7 \tTrain Loss: 0.9225\tValid Loss: 0.9046\tTrain Acc: 0.5465\tValid Acc: 0.5603\n",
      "Epoch: 8 \tTrain Loss: 0.9138\tValid Loss: 0.9013\tTrain Acc: 0.5513\tValid Acc: 0.5674\n",
      "Epoch: 9 \tTrain Loss: 0.9075\tValid Loss: 0.8965\tTrain Acc: 0.5560\tValid Acc: 0.5664\n",
      "Epoch: 10 \tTrain Loss: 0.9028\tValid Loss: 0.8919\tTrain Acc: 0.5593\tValid Acc: 0.5713\n",
      "Epoch: 11 \tTrain Loss: 0.8989\tValid Loss: 0.8927\tTrain Acc: 0.5614\tValid Acc: 0.5683\n",
      "Epoch: 12 \tTrain Loss: 0.8970\tValid Loss: 0.8954\tTrain Acc: 0.5649\tValid Acc: 0.5718\n",
      "Epoch: 13 \tTrain Loss: 0.8934\tValid Loss: 0.8920\tTrain Acc: 0.5679\tValid Acc: 0.5713\n",
      "Epoch: 14 \tTrain Loss: 0.8925\tValid Loss: 0.8890\tTrain Acc: 0.5678\tValid Acc: 0.5716\n",
      "Epoch: 15 \tTrain Loss: 0.8894\tValid Loss: 0.8864\tTrain Acc: 0.5699\tValid Acc: 0.5722\n",
      "Epoch: 16 \tTrain Loss: 0.8850\tValid Loss: 0.8828\tTrain Acc: 0.5737\tValid Acc: 0.5756\n",
      "Epoch: 17 \tTrain Loss: 0.8838\tValid Loss: 0.8895\tTrain Acc: 0.5749\tValid Acc: 0.5721\n",
      "Epoch: 18 \tTrain Loss: 0.8814\tValid Loss: 0.8822\tTrain Acc: 0.5767\tValid Acc: 0.5833\n",
      "Epoch: 19 \tTrain Loss: 0.8794\tValid Loss: 0.8821\tTrain Acc: 0.5785\tValid Acc: 0.5797\n",
      "Epoch: 20 \tTrain Loss: 0.8783\tValid Loss: 0.8774\tTrain Acc: 0.5799\tValid Acc: 0.5777\n",
      "Epoch: 21 \tTrain Loss: 0.8764\tValid Loss: 0.8862\tTrain Acc: 0.5820\tValid Acc: 0.5832\n",
      "Epoch: 22 \tTrain Loss: 0.8741\tValid Loss: 0.8802\tTrain Acc: 0.5834\tValid Acc: 0.5829\n",
      "Epoch: 23 \tTrain Loss: 0.8734\tValid Loss: 0.8797\tTrain Acc: 0.5859\tValid Acc: 0.5746\n",
      "Epoch: 24 \tTrain Loss: 0.8728\tValid Loss: 0.8825\tTrain Acc: 0.5861\tValid Acc: 0.5778\n",
      "Epoch: 25 \tTrain Loss: 0.9082\tValid Loss: 0.9653\tTrain Acc: 0.5593\tValid Acc: 0.5389\n",
      "Epoch: 26 \tTrain Loss: 0.9248\tValid Loss: 0.9105\tTrain Acc: 0.5509\tValid Acc: 0.5647\n",
      "Epoch: 27 \tTrain Loss: 0.9318\tValid Loss: 0.9223\tTrain Acc: 0.5460\tValid Acc: 0.5498\n",
      "Epoch: 28 \tTrain Loss: 0.9029\tValid Loss: 0.9365\tTrain Acc: 0.5729\tValid Acc: 0.5465\n",
      "Epoch: 29 \tTrain Loss: 0.9028\tValid Loss: 0.8848\tTrain Acc: 0.5737\tValid Acc: 0.5885\n",
      "Epoch: 30 \tTrain Loss: 0.8874\tValid Loss: 0.8896\tTrain Acc: 0.5884\tValid Acc: 0.5883\n"
     ]
    }
   ],
   "source": [
    "best_sd = train(30, model, train_loader, test_loader, optimizer, criterion, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1f20fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN:\n",
      "Training Accuracy = 0.5914583333333333\n",
      "Testing Accuracy = 0.5885\n"
     ]
    }
   ],
   "source": [
    "model_best_rnn = model_RNN(20, 2)\n",
    "model_best_rnn.load_state_dict(best_sd)\n",
    "\n",
    "_, train_acc = get_acc(model_best_rnn(X_train_tensor_3d), y_train_tensor)\n",
    "_, test_acc = get_acc(model_best_rnn(X_test_tensor_3d), y_test_tensor)\n",
    "\n",
    "print('RNN:')\n",
    "print('Training Accuracy =', train_acc)\n",
    "print('Testing Accuracy =', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113e305",
   "metadata": {},
   "source": [
    "### 5(b). GRU: Gated Recurrent Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e5fd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_GRU(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_layers):\n",
    "        super(model_GRU, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.GRU(input_size=300, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True, bias=True, bidirectional=False)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, _ = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f435a0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_GRU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3533d7bfe215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initialize the NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_GRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# specify loss function (categorical cross-entropy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_GRU' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize the NN\n",
    "model = model_GRU(20, 2)\n",
    "print(model)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2eea3140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTrain Loss: 1.0956\tValid Loss: 1.0796\tTrain Acc: 0.3530\tValid Acc: 0.3924\n",
      "Epoch: 2 \tTrain Loss: 0.9740\tValid Loss: 0.8967\tTrain Acc: 0.5028\tValid Acc: 0.5582\n",
      "Epoch: 3 \tTrain Loss: 0.9000\tValid Loss: 0.8669\tTrain Acc: 0.5615\tValid Acc: 0.5842\n",
      "Epoch: 4 \tTrain Loss: 0.8690\tValid Loss: 0.8438\tTrain Acc: 0.5881\tValid Acc: 0.5987\n",
      "Epoch: 5 \tTrain Loss: 0.8463\tValid Loss: 0.8308\tTrain Acc: 0.6048\tValid Acc: 0.6096\n",
      "Epoch: 6 \tTrain Loss: 0.8312\tValid Loss: 0.8206\tTrain Acc: 0.6156\tValid Acc: 0.6178\n",
      "Epoch: 7 \tTrain Loss: 0.8196\tValid Loss: 0.8127\tTrain Acc: 0.6219\tValid Acc: 0.6248\n",
      "Epoch: 8 \tTrain Loss: 0.8102\tValid Loss: 0.8071\tTrain Acc: 0.6273\tValid Acc: 0.6273\n",
      "Epoch: 9 \tTrain Loss: 0.8025\tValid Loss: 0.8028\tTrain Acc: 0.6317\tValid Acc: 0.6278\n",
      "Epoch: 10 \tTrain Loss: 0.7959\tValid Loss: 0.7995\tTrain Acc: 0.6341\tValid Acc: 0.6298\n",
      "Epoch: 11 \tTrain Loss: 0.7902\tValid Loss: 0.7967\tTrain Acc: 0.6374\tValid Acc: 0.6312\n",
      "Epoch: 12 \tTrain Loss: 0.7849\tValid Loss: 0.7944\tTrain Acc: 0.6405\tValid Acc: 0.6330\n",
      "Epoch: 13 \tTrain Loss: 0.7801\tValid Loss: 0.7924\tTrain Acc: 0.6433\tValid Acc: 0.6346\n",
      "Epoch: 14 \tTrain Loss: 0.7756\tValid Loss: 0.7907\tTrain Acc: 0.6467\tValid Acc: 0.6358\n",
      "Epoch: 15 \tTrain Loss: 0.7713\tValid Loss: 0.7892\tTrain Acc: 0.6489\tValid Acc: 0.6368\n",
      "Epoch: 16 \tTrain Loss: 0.7672\tValid Loss: 0.7880\tTrain Acc: 0.6519\tValid Acc: 0.6372\n",
      "Epoch: 17 \tTrain Loss: 0.7633\tValid Loss: 0.7869\tTrain Acc: 0.6542\tValid Acc: 0.6377\n",
      "Epoch: 18 \tTrain Loss: 0.7595\tValid Loss: 0.7861\tTrain Acc: 0.6562\tValid Acc: 0.6379\n",
      "Epoch: 19 \tTrain Loss: 0.7558\tValid Loss: 0.7855\tTrain Acc: 0.6581\tValid Acc: 0.6385\n",
      "Epoch: 20 \tTrain Loss: 0.7522\tValid Loss: 0.7850\tTrain Acc: 0.6599\tValid Acc: 0.6395\n",
      "Epoch: 21 \tTrain Loss: 0.7487\tValid Loss: 0.7848\tTrain Acc: 0.6622\tValid Acc: 0.6395\n",
      "Epoch: 22 \tTrain Loss: 0.7452\tValid Loss: 0.7847\tTrain Acc: 0.6641\tValid Acc: 0.6396\n",
      "Epoch: 23 \tTrain Loss: 0.7418\tValid Loss: 0.7848\tTrain Acc: 0.6662\tValid Acc: 0.6394\n",
      "Epoch: 24 \tTrain Loss: 0.7385\tValid Loss: 0.7850\tTrain Acc: 0.6676\tValid Acc: 0.6401\n",
      "Epoch: 25 \tTrain Loss: 0.7352\tValid Loss: 0.7853\tTrain Acc: 0.6696\tValid Acc: 0.6397\n",
      "Epoch: 26 \tTrain Loss: 0.7319\tValid Loss: 0.7858\tTrain Acc: 0.6703\tValid Acc: 0.6395\n",
      "Epoch: 27 \tTrain Loss: 0.7287\tValid Loss: 0.7863\tTrain Acc: 0.6720\tValid Acc: 0.6387\n",
      "Epoch: 28 \tTrain Loss: 0.7255\tValid Loss: 0.7869\tTrain Acc: 0.6741\tValid Acc: 0.6390\n",
      "Epoch: 29 \tTrain Loss: 0.7223\tValid Loss: 0.7876\tTrain Acc: 0.6755\tValid Acc: 0.6372\n",
      "Epoch: 30 \tTrain Loss: 0.7192\tValid Loss: 0.7884\tTrain Acc: 0.6780\tValid Acc: 0.6362\n"
     ]
    }
   ],
   "source": [
    "best_sd = train(30, model, train_loader, test_loader, optimizer, criterion, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2c7678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU:\n",
      "Training Accuracy = 0.6696041666666667\n",
      "Testing Accuracy = 0.6400833333333333\n"
     ]
    }
   ],
   "source": [
    "model_best_gru = model_GRU(20, 2)\n",
    "model_best_gru.load_state_dict(best_sd)\n",
    "\n",
    "_, train_acc = get_acc(model_best_gru(X_train_tensor_3d), y_train_tensor)\n",
    "_, test_acc = get_acc(model_best_gru(X_test_tensor_3d), y_test_tensor)\n",
    "\n",
    "print('GRU:')\n",
    "print('Training Accuracy =', train_acc)\n",
    "print('Testing Accuracy =', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f7c84",
   "metadata": {},
   "source": [
    "### 5(c). LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1cfc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_LSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_layers):\n",
    "        super(model_LSTM, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.LSTM(input_size=300, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True, bias=True, bidirectional=False)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        h0, c0 = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        h0 = torch.randn(self.n_layers, batch_size, self.hidden_dim)\n",
    "        c0 = torch.randn(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return h0, c0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ccc00808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_LSTM(\n",
      "  (rnn): LSTM(300, 20, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize the NN\n",
    "model = model_LSTM(20, 2)\n",
    "print(model)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00835992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTrain Loss: 1.0992\tValid Loss: 1.0988\tTrain Acc: 0.3311\tValid Acc: 0.3333\n",
      "Epoch: 2 \tTrain Loss: 1.0981\tValid Loss: 1.0959\tTrain Acc: 0.3412\tValid Acc: 0.3375\n",
      "Epoch: 3 \tTrain Loss: 1.0347\tValid Loss: 0.9716\tTrain Acc: 0.4532\tValid Acc: 0.5158\n",
      "Epoch: 4 \tTrain Loss: 0.9684\tValid Loss: 0.9413\tTrain Acc: 0.5137\tValid Acc: 0.5275\n",
      "Epoch: 5 \tTrain Loss: 0.9424\tValid Loss: 0.9254\tTrain Acc: 0.5326\tValid Acc: 0.5370\n",
      "Epoch: 6 \tTrain Loss: 0.9230\tValid Loss: 0.8986\tTrain Acc: 0.5487\tValid Acc: 0.5666\n",
      "Epoch: 7 \tTrain Loss: 0.9079\tValid Loss: 0.8851\tTrain Acc: 0.5671\tValid Acc: 0.5785\n",
      "Epoch: 8 \tTrain Loss: 0.8925\tValid Loss: 0.8725\tTrain Acc: 0.5758\tValid Acc: 0.5877\n",
      "Epoch: 9 \tTrain Loss: 0.8816\tValid Loss: 0.8616\tTrain Acc: 0.5830\tValid Acc: 0.5968\n",
      "Epoch: 10 \tTrain Loss: 0.8729\tValid Loss: 0.8581\tTrain Acc: 0.5909\tValid Acc: 0.5996\n",
      "Epoch: 11 \tTrain Loss: 0.8665\tValid Loss: 0.8529\tTrain Acc: 0.5940\tValid Acc: 0.6028\n",
      "Epoch: 12 \tTrain Loss: 0.8591\tValid Loss: 0.8476\tTrain Acc: 0.5978\tValid Acc: 0.6023\n",
      "Epoch: 13 \tTrain Loss: 0.8524\tValid Loss: 0.8433\tTrain Acc: 0.6020\tValid Acc: 0.6062\n",
      "Epoch: 14 \tTrain Loss: 0.8480\tValid Loss: 0.8415\tTrain Acc: 0.6031\tValid Acc: 0.6112\n",
      "Epoch: 15 \tTrain Loss: 0.8424\tValid Loss: 0.8362\tTrain Acc: 0.6072\tValid Acc: 0.6129\n",
      "Epoch: 16 \tTrain Loss: 0.8376\tValid Loss: 0.8377\tTrain Acc: 0.6108\tValid Acc: 0.6113\n",
      "Epoch: 17 \tTrain Loss: 0.8332\tValid Loss: 0.8318\tTrain Acc: 0.6146\tValid Acc: 0.6125\n",
      "Epoch: 18 \tTrain Loss: 0.8307\tValid Loss: 0.8308\tTrain Acc: 0.6138\tValid Acc: 0.6151\n",
      "Epoch: 19 \tTrain Loss: 0.8261\tValid Loss: 0.8311\tTrain Acc: 0.6173\tValid Acc: 0.6107\n",
      "Epoch: 20 \tTrain Loss: 0.8232\tValid Loss: 0.8274\tTrain Acc: 0.6183\tValid Acc: 0.6150\n",
      "Epoch: 21 \tTrain Loss: 0.8191\tValid Loss: 0.8293\tTrain Acc: 0.6205\tValid Acc: 0.6119\n",
      "Epoch: 22 \tTrain Loss: 0.8165\tValid Loss: 0.8233\tTrain Acc: 0.6230\tValid Acc: 0.6143\n",
      "Epoch: 23 \tTrain Loss: 0.8115\tValid Loss: 0.8193\tTrain Acc: 0.6262\tValid Acc: 0.6166\n",
      "Epoch: 24 \tTrain Loss: 0.8096\tValid Loss: 0.8200\tTrain Acc: 0.6284\tValid Acc: 0.6213\n",
      "Epoch: 25 \tTrain Loss: 0.8069\tValid Loss: 0.8190\tTrain Acc: 0.6291\tValid Acc: 0.6191\n",
      "Epoch: 26 \tTrain Loss: 0.8051\tValid Loss: 0.8164\tTrain Acc: 0.6305\tValid Acc: 0.6252\n",
      "Epoch: 27 \tTrain Loss: 0.7997\tValid Loss: 0.8219\tTrain Acc: 0.6317\tValid Acc: 0.6158\n",
      "Epoch: 28 \tTrain Loss: 0.7972\tValid Loss: 0.8184\tTrain Acc: 0.6322\tValid Acc: 0.6203\n",
      "Epoch: 29 \tTrain Loss: 0.7956\tValid Loss: 0.8148\tTrain Acc: 0.6350\tValid Acc: 0.6215\n",
      "Epoch: 30 \tTrain Loss: 0.7926\tValid Loss: 0.8166\tTrain Acc: 0.6354\tValid Acc: 0.6200\n"
     ]
    }
   ],
   "source": [
    "best_sd = train(30, model, train_loader, test_loader, optimizer, criterion, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32b4dd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM:\n",
      "Training Accuracy = 0.6322708333333333\n",
      "Testing Accuracy = 0.6245\n"
     ]
    }
   ],
   "source": [
    "model_best_lstm = model_LSTM(20, 2)\n",
    "model_best_lstm.load_state_dict(best_sd)\n",
    "\n",
    "_, train_acc = get_acc(model_best_lstm(X_train_tensor_3d), y_train_tensor)\n",
    "_, test_acc = get_acc(model_best_lstm(X_test_tensor_3d), y_test_tensor)\n",
    "\n",
    "print('LSTM:')\n",
    "print('Training Accuracy =', train_acc)\n",
    "print('Testing Accuracy =', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0d108",
   "metadata": {},
   "source": [
    "> What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN? <br>\n",
    "> - GRU gives the best test accuracy at 64% while LSTM's accuracy is slightly lower at 62.5%. For simple RNN, the best test accuracy achieved is at 58.9%. This can be inferred that this sentiment prediction task also relies on the long-term dependency of the input reviews, and LSTM and GRU are doing better task in storing and retrieving long-term dependency than simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f055a",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "- https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook\n",
    "- https://stackoverflow.com/questions/3438756/some-built-in-to-pad-a-list-in-python\n",
    "- https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
    "- https://androidkt.com/copy-pytorch-model-using-deepcopy-and-state_dict/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
