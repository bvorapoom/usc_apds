{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb92235b",
   "metadata": {},
   "source": [
    "<center><h1>CSCI-544 HOMEWORK 2</h1>\n",
    "<br>\n",
    "<center><font size=\"3\">Name: Vorapoom Thirapatarapong</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2a546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import task1 as t1\n",
    "import task2 as t2\n",
    "import task3 as t3\n",
    "import task4 as t4\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4158e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'data/train'\n",
    "path_dev = 'data/dev'\n",
    "path_test = 'data/test'\n",
    "occ_thres = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4bb494",
   "metadata": {},
   "source": [
    "# Task1\n",
    "- What is the selected threshold for unknown words replacement? <br>\n",
    "The selected threshold for unknown words replacement used is 0, meaning that I chose to not replace any word with low frequency to be unknown. This is because the threshold at 0 gives the highest accuracy on the dev data on both greedy and viterbi decoding on task3 and task4. Results of different thresholds used can be seen below.\n",
    "- What is the total size of your vocabulary? <br>\n",
    "The total size of vocaulary is 43,193 words.\n",
    "- What is the total occurrences of the special token ‘< unk >’ after replacement? <br>\n",
    "As mentioned earlier that I chose to use the unknown words threshold of 0, there are 0 occurrences of the special token <unk> in my vocabulary.\n",
    "- Explanation <br>\n",
    "Task1 was done in 2 simple steps. First, each word of the input file is kept in a dictionary along with its occurrences. Next, each word in the dictionary is checked whether its occurrences are above the threshold. If not, those words will be added to the < unk > token instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cca1d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold of unknown word replacement = 0\n",
      "Total size of vocabulary = 43193\n",
      "Total occurrences of special token < unk > after replacement = 0\n"
     ]
    }
   ],
   "source": [
    "corpus = t1.create_corpus(path_train, occ_thres)\n",
    "t1.query(corpus, occ_thres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c8ce7",
   "metadata": {},
   "source": [
    "# Task2\n",
    "- How many transition parameters in your HMM? <br>\n",
    "There are 1,392 transition parameters. These include the initial transition parameters as well.\n",
    "- How many emission parameters in your HMM? <br>\n",
    "There are 50,286 emission parameters in total.\n",
    "- Explanation <br>\n",
    "Firstly, the train data is transformed to a list of lists for ease of processing (first layer represents each sentence and the layer inside represents each word in a particular sentence). Then, the nested for-loop is performed to collect the information of transition and emission parameters. When these raw values are collected, the final values which are joint/conditional probabilities are calculated. Note that 2 versions of emission parameters are being collected: the original word, and a lowercase word which will be used when training the greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5424c385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transition parameters = 1392\n",
      "Number of emission parameters = 50286\n"
     ]
    }
   ],
   "source": [
    "data = t2.read_file_to_list(path_train)\n",
    "transition, emission, list_pos, emission_lower = t2.get_emission_transition(data)\n",
    "t2.query(transition, emission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1e8aa",
   "metadata": {},
   "source": [
    "# Task3\n",
    "- What is the accuracy of Greedy Decoding on the dev data? <br>\n",
    "The highest accuracy achieved on dev data is 93.78%\n",
    "- Explanation <br>\n",
    "In the greedy decoding, when a new sentence comes in for a prediction, firstly the code will predict the POS of the first word based on the product of the emission prob and the initial transition prob if this first word is in the corpus. If not, the code will try to check if the lowercase of this first word is in corpus (found a few cases where this is the case and it helps improve the accuracy) and find POS based on the same method. If none of the above cases happen, POS prediction will be NNP. After the prediction of the first word, subsequent words' POS are predicted similarly using the product of the emission prob and the transition prob. However, if the word is not in the corpus (emission prob = 0), the POS predicted will be the one that gives the highest transition probability based on the previous word's POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c34cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = t2.read_file_to_list(path_train)\n",
    "dev = t2.read_file_to_list(path_dev)\n",
    "corpus = t1.create_corpus(path_train, occ_thres)\n",
    "corpus = list(corpus.keys())\n",
    "transition, emission, list_pos, emission_lower = t2.get_emission_transition(train)\n",
    "transition_init = {k: v for k, v in transition.items() if k[0] == 'start'}\n",
    "transition_def = t3.get_transition_default(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eec993c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Decoding: Accuracy on dev data = 0.937799769291482\n"
     ]
    }
   ],
   "source": [
    "prediction_dev = t3.make_prediction(dev, transition, emission, list_pos, corpus, transition_def, emission_lower, transition_init)\n",
    "acc = t3.get_accuracy(dev, prediction_dev)\n",
    "print('Greedy Decoding: Accuracy on dev data =', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc3268",
   "metadata": {},
   "source": [
    "# Task4\n",
    "- What is the accuracy of Viterbi Decoding on the dev data? <br>\n",
    "The highest accuracy achieved on dev data is 94.36%\n",
    "- Explanation <br>\n",
    "In the Viterbi decoding, when a new sentence comes in for a prediction, firstly the code will calculate the probability of each POS of the first word from the product of emission and initial transition probabilities in a dictionary. The code will only store the values for POS that exist for that first word in the emission parameters. However, if the word is not in the corpus, the default dictionary will be used (initial transition parameters). For the subsequent words, for each of the possible POS, the dictionary will be updated to store the maximum probability of each of the previous words' POS. Finally, the sequence that gives the highest probability at the end will be used as a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be40e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_init = {k[1]: (v, [k[1]]) for k, v in transition.items() if k[0] == 'start'}\n",
    "transition_def = t4.get_transition_default(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2698f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Decoding: Accuracy on dev data = 0.9436206059134236\n"
     ]
    }
   ],
   "source": [
    "prediction_dev = t4.make_prediction(dev, transition, emission, list_pos, corpus, transition_def, emission_lower, transition_init)\n",
    "acc = t4.get_accuracy(dev, prediction_dev)\n",
    "print('Viterbi Decoding: Accuracy on dev data =', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc0049",
   "metadata": {},
   "source": [
    "### Test different threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862c6aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold performance variation: greedy decoding\n",
      "threshold: 0 , dev accuracy:  0.937799769291482\n",
      "threshold: 1 , dev accuracy:  0.9298919312731467\n",
      "threshold: 3 , dev accuracy:  0.9193051423714407\n",
      "threshold: 5 , dev accuracy:  0.9105245583146135\n",
      "threshold: 10 , dev accuracy:  0.892121000546415\n",
      "threshold: 20 , dev accuracy:  0.8661890595592253\n"
     ]
    }
   ],
   "source": [
    "# greedy\n",
    "train = t2.read_file_to_list(path_train)\n",
    "dev = t2.read_file_to_list(path_dev)\n",
    "test = t2.read_file_to_list(path_test)\n",
    "\n",
    "thres_list = [0, 1, 3, 5, 10, 20]\n",
    "print('Threshold performance variation: greedy decoding')\n",
    "for thres in thres_list: \n",
    "    # get corpus\n",
    "    corpus = t1.create_corpus(path_train, thres)\n",
    "    corpus = list(corpus.keys())\n",
    "\n",
    "    # get transition / emission parameters\n",
    "    transition, emission, list_pos, emission_lower = t2.get_emission_transition(train)\n",
    "    transition_init = {k: v for k, v in transition.items() if k[0] == 'start'}\n",
    "    transition_def = t3.get_transition_default(transition)\n",
    "\n",
    "\n",
    "    # predict dev\n",
    "    prediction_dev = t3.make_prediction(dev, transition, emission, list_pos, corpus, transition_def, emission_lower, transition_init)\n",
    "\n",
    "    # get accuracy\n",
    "    acc = t3.get_accuracy(dev, prediction_dev)\n",
    "    print('threshold:', thres, ', dev accuracy: ', acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c5183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold performance variation: viterbi decoding\n",
      "threshold: 0 , dev accuracy:  0.9436206059134236\n",
      "threshold: 1 , dev accuracy:  0.9349310910084391\n",
      "threshold: 3 , dev accuracy:  0.9223863153421165\n",
      "threshold: 5 , dev accuracy:  0.9128544107825876\n",
      "threshold: 10 , dev accuracy:  0.8925535790176674\n",
      "threshold: 20 , dev accuracy:  0.8641703600267137\n"
     ]
    }
   ],
   "source": [
    "# viterbi\n",
    "\n",
    "thres_list = [0, 1, 3, 5, 10, 20]\n",
    "print('Threshold performance variation: viterbi decoding')\n",
    "for thres in thres_list:\n",
    "    # get corpus\n",
    "    corpus = t1.create_corpus(path_train, thres)\n",
    "    corpus = list(corpus.keys())\n",
    "\n",
    "    # get transition / emission parameters\n",
    "    transition, emission, list_pos, emission_lower = t2.get_emission_transition(train)\n",
    "    transition_init = {k[1]: (v, [k[1]]) for k, v in transition.items() if k[0] == 'start'}\n",
    "    transition_def = t4.get_transition_default(transition)\n",
    "\n",
    "\n",
    "    # predict\n",
    "    prediction_dev = t4.make_prediction(dev, transition, emission, list_pos, corpus, transition_def, emission_lower, transition_init)\n",
    "\n",
    "    # get accuracy\n",
    "    acc = t4.get_accuracy(dev, prediction_dev)\n",
    "    print('threshold:', thres, ', dev accuracy: ', acc)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
